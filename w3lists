#!/usr/bin/env python
import argparse
from contextlib import closing
import datetime
import email
import glob
import io
import json
import logging
import mailbox
from operator import attrgetter
import os
import re
import rfc822
import shutil
import sys
import urllib2
from xml.sax.saxutils import escape
#from xml.etree import ElementTree
from lxml import etree
logger = logging.getLogger('w3lists')
logging.basicConfig()

class W3lists(object):
	def __init__(self):
		self.message_by_id = {}
		self._message_by_url = None
		self.normalized_url = {}

	@property
	def messages(self):
		return self.message_by_id.itervalues()

	def message_by_url(self, url):
		if not self._message_by_url:
			self._message_by_url = {}
			for m in self.messages:
				self._message_by_url[m.archived_at] = m
		return self._message_by_url.get(url)

	def download_mbox(self, dir, month_index_range, user, password):
		"""Download mbox files from W3C member server at https://lists.w3.org/Archives/Public/${list}/mboxes/${year}-${month}.mbx"""
		auth_handler = urllib2.HTTPBasicAuthHandler()
		auth_handler.add_password('W3CACL', 'https://lists.w3.org/', user, password)
		opener = urllib2.build_opener(auth_handler)
		for filename in (self.mbox_filename_from_month_index(i) for i in month_index_range):
			logger.info('Downloading %s', filename)
			url = 'https://lists.w3.org/Archives/Public/www-style/mboxes/' + filename
			with closing(opener.open(url)) as rs:
				with open(os.path.join(dir, filename), 'wb') as fp:
					shutil.copyfileobj(rs, fp)

	@staticmethod
	def mbox_filename_from_month_index(month_index):
		return '%04d-%02d.mbx' % (month_index / 12, month_index % 12 + 1)

	def month_index_min(self, dir):
		"""Get the month_index of the oldest mbox file to download"""
		files = glob.glob(os.path.join(dir, '*.mbx'))
		if not files:
			return None
		filename = last(sorted(os.path.splitext(os.path.basename(f))[0] for f in files))
		(year, month) = (int(s) for s in filename.split('-'))
		return year * 12 + month - 1

	def load_mbox(self, path, filter =None):
		path = os.path.expanduser(path)
		if os.path.isdir(path):
			for file in glob.glob(os.path.join(path, '*.mbx')):
				self.load_mbox(file, filter)
			return
		logger.info('Loading %s', path)
		mbox = mailbox.mbox(path)
		for mbmsg in mbox:
			m = W3lists.Message(mbmsg)
			if filter and not filter(m):
				continue
			m.archived_at = self.normalize_url(m.archived_at)
			self.message_by_id[m.id] = m

	def load_urls(self, path):
		logger.info('Loading URLs from %s', path)
		with open(path, 'r') as f:
			self.normalized_url = json.load(f)

	def save_urls(self, path):
		logger.info('Saving URLs to %s', path)
		with open(path, 'w') as f:
			json.dump(self.normalized_url, f, indent=0)

	def normalize_url(self, url):
		normalized = self.normalized_url.get(url)
		if normalized:
			return normalized
		try:
			redir = get_redirect_url(url)
			if not redir:
				return url
		except urllib2.HTTPError as err:
			if err.code == 404:
				return url
			raise
		self.normalized_url[url] = redir
		return redir

	def link_replies(self):
		logger.info("Linking replies...")
		for m in self.message_by_id.itervalues():
			m.link_in_reply_to(self.message_by_id)
		self.link_replies_by_subject()

	def link_replies_by_subject(self):
		logger.info("Linking replies using subjects...")
		message_by_subject = {}
		re_subject_prefix = re.compile('^[a-zA-Z]{2,3}:\s*')
		def normalize_subject(subject):
			subject = re_subject_prefix.sub('', subject)
			subject = subject.replace(' ', '')
			return subject
		for m in self.message_by_id.itervalues():
			subject = normalize_subject(m.subject)
			thread = message_by_subject.get(subject)
			if thread:
				thread.append(m)
			else:
				message_by_subject[subject] = [m]
		for m in filter(lambda m: not m.in_reply_to, self.message_by_id.itervalues()):
			subject = m.subject
			if not re_subject_prefix.match(subject):
				continue
			subject = normalize_subject(subject)
			thread = message_by_subject.get(subject)
			assert thread
			earliers = filter(lambda mm: mm.date < m.date, thread)
			if not earliers:
				logger.info('Link maybe-by-subject candidates not found: %s from %s on %s', m.subject, m.from_display, m.date)
				continue
			in_reply_to = first(reversed(sorted(earliers, key=attrgetter('date'))))
			assert normalize_subject(m.subject) == normalize_subject(in_reply_to.subject)
			logger.info('Linking maybe-by-subject: %s from %s on %s', m.subject, m.from_display, m.date)
			in_reply_to.add_reply(m)
			assert m.in_reply_to

	def dump_reply_tree(self, output):
		output.write('''<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
</head>
<body>
<ol>''')
		for m in sorted(filter(lambda m: m.in_reply_to is None, self.message_by_id.itervalues()), key=attrgetter('date')):
			m.dump_reply_tree(output)
		output.write('</ol></body></html>\n')

	class Message(object):
		def __init__(self, mbmsg):
			self.archived_at = strip_angle_brackets(mbmsg['archived-at'])
			# Protect from a bug where archive-at can contain display names such as:
			# http://www.w3.org/mid/896B66B790C04549AD3BF12412E175E39D2A944772@hst-mail01.sdbit.local(sfid-20140212_141727_830873_C9E64B6E)
			if self.archived_at and self.archived_at.endswith(')'):
				i = self.archived_at.rfind('(')
				if i > 0:
					self.archived_at = self.archived_at[0:i]
			self.date = datetime.datetime.fromtimestamp(rfc822.mktime_tz(rfc822.parsedate_tz(mbmsg['date'])))
			(self.from_display, self.from_address) = rfc822.parseaddr(decode_header_to_unicode(mbmsg['from']))
			self.id = email.Utils.parseaddr(mbmsg['message-id'])[1]
			self.in_reply_to_id = mbmsg['in-reply-to']
			if self.in_reply_to_id:
				self.in_reply_to_id = email.Utils.parseaddr(self.in_reply_to_id)[1]
			self.subject = mbmsg['subject']
			self.in_reply_to = None
			self.replies = []
			self.issues = []

		def link_in_reply_to(self, message_by_id):
			assert not self.in_reply_to
			in_reply_to_id = self.in_reply_to_id
			if not in_reply_to_id:
				return
			in_reply_to = message_by_id.get(in_reply_to_id)
			if not in_reply_to:
				logger.warn('Message id %s not loaded', in_reply_to_id)
				return
			in_reply_to.add_reply(self)

		def add_reply(self, reply):
			assert not reply.in_reply_to
			self.replies.append(reply)
			reply.in_reply_to = self

		def dump_reply_tree(self, output):
			output.write('\n<li><a href="{href}">{subject}</a> {from_display} {date}'.format(
				date=self.date,
				from_display=escape(self.from_display),
				href=self.archived_at,
				subject=escape(self.subject)))
			if self.replies:
#				output.write(' %d replies' % len(self.replies))
				output.write('<ol>')
				for r in sorted(self.replies, key=attrgetter('date')):
					r.dump_reply_tree(output)
				output.write('</ol>')
			output.write('</li>')

class W3DoC(object):
	def __init__(self):
		self.prologue = []
		self.issues = []

	def load(self, path):
		with io.open(path, 'r', encoding='utf-8') as file:
			for line in file:
				if line.startswith('---'):
					break
				self.prologue.append(line)
			while True:
				issue = W3DoC.Issue()
				if not issue.load(file):
					break
				self.issues.append(issue)

	def save(self, file):
		if isinstance(file, basestring):
			with io.open(file, 'w', encoding='utf-8') as f:
				self.save(f)
			return
		for line in self.prologue:
			file.write(line)
		for issue in self.issues:
			issue.save(file)

	def link_messages(self, list):
		for issue in self.issues:
			issue.link_messages(list)

	def add_issues(self, list):
		id = 0
		for m in sorted(filter(lambda m: not m.in_reply_to and not m.issues, list.messages), key=attrgetter('date')):
			issue = W3DoC.Issue()
			if id != 0:
				id += 1
			else:
				id = max(self.issues, key=lambda i: i.id).id + 1
			issue.id = id
			issue.from_name = m.from_display
			issue.summary = m.subject
			issue.thread.append(['Comment', m.archived_at, m])
			logger.info('New issue %d: %s from %s', issue.id, issue.summary, issue.from_name)
			self.issues.append(issue)

	def add_messages(self, list):
		for issue in self.issues:
			issue.add_messages(list)

	class Issue(object):
		def __init__(self):
			self.id = None
			self.from_name = None
			self.resolution = None
			self.status = 'Open'
			self.summary = None
			self.thread = []

		def load(self, file):
			line = file.readline()
			if not line:
				return False
			match = re.match(r'Issue (\d+)', line)
			if not match:
				raise Exception('Issue must start with an issue #')
			self.id = int(match.group(1))
			for line in file:
				if line.startswith('---'):
					break
				(key, value) = re.split(r':\s*', line.strip(), 1)
				if key == 'Summary':
					self.summary = value
				elif key == 'From':
					self.from_name = value
				elif key == 'Comment' or key == 'Response':
					self.thread.append([key, value, None])
				elif key == 'Open' or key == 'Closed':
					self.status = key
					self.resolution = value
				else:
					raise Exception('Unknown key "%s" ignored' % key)
			return True

		def save(self, file):
			file.write(u'----\nIssue %d.\n' % self.id)
			for t in self.get_key_value_pairs():
				if not t[1]:
					file.write(unicode(t[0] + ':\n'))
				else:
					file.write(unicode((t[0] + ':').ljust(10) + t[1] + '\n'))

		def get_key_value_pairs(self):
			yield ('Summary', self.summary)
			yield ('From', self.from_name)
			for t in self.thread:
				yield (t[0], t[1])
			yield (self.status, self.resolution)

		def link_messages(self, list):
			for t in self.thread:
				m = list.message_by_url(t[1])
				t[2] = m
				if not m:
					logger.warn('Issue %d: %s %s not loaded', self.id, t[0], t[1])
					continue
				m.issues.append(self)

		def add_messages(self, list):
			t = first(reversed(filter(lambda t: t[2], self.thread)))
			if not t:
				logger.warn('Issue %d: No messages found' % self.id)
				return
			m = t[2]
			replies = []
			self._get_messages(m, replies)
			for reply in sorted(replies, key=attrgetter('date')):
				logger.info('Issue %d: add comment %s' % (self.id, reply.archived_at))
				self.thread.append(['Comment', reply.archived_at, reply])

		def _get_messages(self, m, replies):
			for reply in sorted(m.replies, key=attrgetter('date')):
				if reply.issues: # do not include messages with assigned issues in case issues were branched
					continue
				logger.info('Issue %d: add comment %s' % (self.id, reply.archived_at))
				replies.append(reply)
				self._get_messages(reply, replies)

def get_redirect_url(url):
	try:
		with closing(urllib2.urlopen(url)) as response:
			redir = response.geturl()
		if redir != url:
			logger.debug('Normalized %s from %s', redir, url)
			return redir
		return None
	except urllib2.HTTPError as err:
		if err.code == 300:
			logger.debug('300 from %s', url)
			t = etree.parse(err)
			etree.tostring(t)
			hrefs = map(lambda a: a.attrib['href'], t.findall('.//li/a'))
			for href in hrefs:
				if 'www-style' in href:
					return href
			return hrefs[0]
		logger.error('HTTP %d for %s', err.code, url)
		raise
#	except:
#		err = sys.exc_info()[0]
#		logger.error('%s: %s', self.archived_at, err)

def decode_header_to_unicode(text):
	decoded = email.header.decode_header(text)
	return u''.join([unicode(text, charset or 'ascii') for text, charset in decoded])

re_angle_bracket = re.compile(r'<([^>]*)>')

def strip_angle_brackets(text):
	match = re_angle_bracket.search(text)
	if match:
		return match.group(1)
	return text

def first(iterable, default=None):
	if iterable:
		for item in iterable:
			return item
	return default

def last(iterable, default=None):
	if iterable:
		for item in iterable:
			default = item
	return default

def main():
	reload(sys).setdefaultencoding('utf-8')
	parser = argparse.ArgumentParser()
	parser.add_argument('--doc')
	parser.add_argument('--exclude', '-x', action='append')
	parser.add_argument('--filter', '-f', action='append')
	parser.add_argument('--mbox', '-m', default='~/Downloads')
	parser.add_argument('--redir', '-r', default='redir.json')
	parser.add_argument('--user', '-u',)
	parser.add_argument('--verbose', '-v', action='append_const', const=1)
#	parser.add_argument('url', nargs='*')
	args = parser.parse_args()
	if args.verbose:
		logger.setLevel(level=logging.INFO if len(args.verbose) == 1 else logging.DEBUG)
	args.mbox = os.path.expanduser(args.mbox)
	list = W3lists()
	if args.user:
		(user, password) = args.user.split(':')
		min = list.month_index_min(args.mbox)
		if min:
			today = datetime.date.today()
			lim = today.year * 12 + today.month
			list.download_mbox(args.mbox, range(min, lim), user, password)
		else:
			info.Warning('Downloading mbox skipped: you need to have at least one mbox file to download')
	if args.redir:
		args.redir = os.path.expanduser(os.path.join(args.mbox, args.redir))
		if os.path.exists(args.redir):
			list.load_urls(args.redir)
	if args.filter:
		f = re.compile('|'.join(args.filter), re.I)
		if args.exclude:
			x = re.compile('|'.join(args.exclude), re.I)
			filter = lambda m: f.search(m.subject) and not x.search(m.subject)
		else:
			filter = lambda m: f.search(m.subject)
	else:
		filter = None
	list.load_mbox(args.mbox, filter)
	list.link_replies()
	if args.redir:
		list.save_urls(args.redir)
	if args.doc:
		doc = W3DoC()
		doc.load(args.doc)
		doc.link_messages(list)
		doc.add_issues(list)
		doc.add_messages(list)
		doc.save(args.doc)
	else:
		list.dump_reply_tree(sys.stdout)
	logger.info("Completed")

main()
