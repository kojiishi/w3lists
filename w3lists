#!/usr/bin/env python
import argparse
from contextlib import closing
import datetime
import email
import glob
import io
import json
import logging
import mailbox
from operator import attrgetter
import os
import re
import rfc822
import shutil
import sys
import urllib2
from xml.sax.saxutils import escape
#from xml.etree import ElementTree
from lxml import etree
logger = logging.getLogger('w3lists')
logging.basicConfig()

class W3lists(object):
	def __init__(self):
		self.message_by_id = {}
		self._message_by_url = None
		self.normalized_url = {}

	@property
	def messages(self):
		return self.message_by_id.itervalues()

	def message_by_url(self, url):
		if not self._message_by_url:
			self._message_by_url = {}
			for m in self.messages:
				self._message_by_url[m.archived_at] = m
		msg = self._message_by_url.get(url)
		if msg:
			return msg
		url = self.normalized_url.get(url)
		if url:
			return self._message_by_url.get(url)
		return None

	def download_mbox(self, dir, month_index_range, user, password):
		"""Download mbox files from W3C member server at https://lists.w3.org/Archives/Public/${list}/mboxes/${year}-${month}.mbx"""
		if isinstance(month_index_range, int):
			month_index_range = range(month_index_range, self.month_index_today())
		auth_handler = urllib2.HTTPBasicAuthHandler()
		auth_handler.add_password('W3CACL', 'https://lists.w3.org/', user, password)
		opener = urllib2.build_opener(auth_handler)
		for filename in (self.mbox_filename_from_month_index(i) for i in month_index_range):
			logger.info('Downloading %s', filename)
			url = 'https://lists.w3.org/Archives/Public/www-style/mboxes/' + filename
			with closing(opener.open(url)) as rs:
				with open(os.path.join(dir, filename), 'wb') as fp:
					shutil.copyfileobj(rs, fp)

	def month_index_range_to_download(self, dir, since):
		files = glob.glob(os.path.join(dir, '*.mbx'))
		existing = set(self.month_index_from_string(os.path.splitext(os.path.basename(f))[0]) for f in files)
		if since:
			min = self.month_index_from_string(since)
		elif not files:
			return None
		else:
			min = min(existing)
		r = range(min, self.month_index_today())
		if not files:
			return r
		existing.remove(max(existing)) # always re-download the latest one
		return (i for i in r if i not in existing)

	@staticmethod
	def mbox_filename_from_month_index(month_index):
		return '%04d-%02d.mbx' % (month_index / 12, month_index % 12 + 1)

	@staticmethod
	def month_index_from_string(value):
		(year, month) = (int(s) for s in value.split('-'))
		return year * 12 + month - 1

	@staticmethod
	def month_index_today():
		today = datetime.date.today()
		return today.year * 12 + today.month

	def load_mbox_since(self, dir, since, filter =None):
		since = self.month_index_from_string(since)
		for filename in (self.mbox_filename_from_month_index(i) for i in range(since, self.month_index_today())):
			self.load_mbox(os.path.join(dir, filename), filter)

	def load_mbox(self, path, filter =None):
		path = os.path.expanduser(path)
		if os.path.isdir(path):
			for file in glob.glob(os.path.join(path, '*.mbx')):
				self.load_mbox(file, filter)
			return
		logger.info('Loading %s', path)
		mbox = mailbox.mbox(path)
		for mbmsg in mbox:
			m = W3lists.Message(mbmsg)
			if filter and not filter(m):
				continue
			m.archived_at = self.normalize_url(m.archived_at)
			self.message_by_id[m.id] = m

	def load_urls(self, path):
		logger.info('Loading URLs from %s', path)
		with open(path, 'r') as f:
			self.normalized_url = json.load(f)

	def save_urls(self, path):
		logger.info('Saving URLs to %s', path)
		with open(path, 'w') as f:
			json.dump(self.normalized_url, f, indent=0)

	def normalize_url(self, url):
		normalized = self.normalized_url.get(url)
		if normalized:
			return normalized
		try:
			redir = get_redirect_url(url)
			if not redir:
				return url
		except urllib2.HTTPError as err:
			if err.code == 404:
				return url
			raise
		self.normalized_url[url] = redir
		return redir

	def link_replies(self):
		logger.info("Linking replies...")
		for m in self.message_by_id.itervalues():
			m.link_in_reply_to(self.message_by_id)
		self.link_replies_by_subject()

	def link_replies_by_subject(self):
		logger.info("Linking replies using subjects...")
		message_by_subject = {}
		re_subject_prefix = re.compile('^[a-zA-Z]{2,3}:\s*')
		def normalize_subject(subject):
			subject = re_subject_prefix.sub('', subject)
			subject = subject.replace(' ', '')
			return subject
		for m in self.message_by_id.itervalues():
			subject = normalize_subject(m.subject)
			thread = message_by_subject.get(subject)
			if thread:
				thread.append(m)
			else:
				message_by_subject[subject] = [m]
		for m in filter(lambda m: not m.in_reply_to, self.message_by_id.itervalues()):
			subject = m.subject
			if not re_subject_prefix.match(subject):
				continue
			subject = normalize_subject(subject)
			thread = message_by_subject.get(subject)
			assert thread
			earliers = filter(lambda mm: mm.date < m.date, thread)
			if not earliers:
				logger.info('Link maybe-by-subject candidates not found: %s from %s on %s', m.subject, m.from_display, m.date)
				continue
			in_reply_to = first(reversed(sorted(earliers, key=attrgetter('date'))))
			assert normalize_subject(m.subject) == normalize_subject(in_reply_to.subject)
			logger.info('Linking maybe-by-subject: %s from %s on %s', m.subject, m.from_display, m.date)
			in_reply_to.add_reply(m)
			assert m.in_reply_to

	def dump_reply_tree(self, output):
		output.write('''<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
</head>
<body>
<ol>''')
		for m in sorted(filter(lambda m: m.in_reply_to is None, self.message_by_id.itervalues()), key=attrgetter('date')):
			m.dump_reply_tree(output)
		output.write('</ol></body></html>\n')

	class Message(object):
		def __init__(self, mbmsg):
			self.archived_at = strip_angle_brackets(mbmsg['archived-at'])
			# Protect from a bug where archive-at can contain display names such as:
			# http://www.w3.org/mid/896B66B790C04549AD3BF12412E175E39D2A944772@hst-mail01.sdbit.local(sfid-20140212_141727_830873_C9E64B6E)
			if self.archived_at and self.archived_at.endswith(')'):
				i = self.archived_at.rfind('(')
				if i > 0:
					self.archived_at = self.archived_at[0:i]
			self.date = datetime.datetime.fromtimestamp(rfc822.mktime_tz(rfc822.parsedate_tz(mbmsg['date'])))
			(self.from_display, self.from_address) = rfc822.parseaddr(decode_header_to_unicode(mbmsg['from']))
			self.id = email.Utils.parseaddr(mbmsg['message-id'])[1]
			self.in_reply_to_id = mbmsg['in-reply-to']
			if self.in_reply_to_id:
				self.in_reply_to_id = email.Utils.parseaddr(self.in_reply_to_id)[1]
			self.subject = mbmsg['subject']
			self.in_reply_to = None
			self.replies = []
			self.issues = []

		def link_in_reply_to(self, message_by_id):
			assert not self.in_reply_to
			in_reply_to_id = self.in_reply_to_id
			if not in_reply_to_id:
				return
			in_reply_to = message_by_id.get(in_reply_to_id)
			if not in_reply_to:
				logger.warn('Message id %s not loaded', in_reply_to_id)
				return
			in_reply_to.add_reply(self)

		def add_reply(self, reply):
			assert not reply.in_reply_to
			self.replies.append(reply)
			reply.in_reply_to = self

		def dump_reply_tree(self, output):
			output.write('\n<li><a href="{href}">{subject}</a> {from_display} {date}'.format(
				date=self.date,
				from_display=escape(self.from_display),
				href=self.archived_at,
				subject=escape(self.subject)))
			if self.replies:
#				output.write(' %d replies' % len(self.replies))
				output.write('<ol>')
				for r in sorted(self.replies, key=attrgetter('date')):
					r.dump_reply_tree(output)
				output.write('</ol>')
			output.write('</li>')

class W3DoC(object):
	def __init__(self):
		self.prologue = []
		self.issues = []

	def load(self, path):
		with io.open(path, 'r', encoding='utf-8') as file:
			for line in file:
				if line.startswith('---'):
					break
				self.prologue.append(line)
			while True:
				issue = W3DoC.Issue.load(file)
				if not issue:
					break
				self.issues.append(issue)

	def save(self, file):
		if isinstance(file, basestring):
			with io.open(file, 'w', encoding='utf-8') as f:
				self.save(f)
			return
		for line in self.prologue:
			file.write(line)
		for issue in self.issues:
			issue.save(file)

	def link_messages(self, list):
		for issue in self.issues:
			issue.link_messages(list)

	def add_issues(self, list):
		id = None
		for m in sorted(filter(lambda m: not m.in_reply_to and not m.issues, list.messages), key=attrgetter('date')):
			if not id:
				id = max(self.issues, key=lambda i: i.id).id
			id += 1
			issue = W3DoC.Issue.from_message(id, m)
			logger.info('New issue %d: %s from %s', issue.id, issue.summary, issue.from_name)
			self.issues.append(issue)

	def add_messages(self, list):
		for issue in self.issues:
			issue.add_messages(list)

	class Issue(object):
		def __init__(self):
			self.id = None
			self.from_name = None
			self.resolution = None
			self.status = 'Open'
			self.summary = None
			self.comments = []

		@staticmethod
		def from_message(id, message):
			issue = W3DoC.Issue()
			issue.id = id
			issue.from_name = message.from_display
			issue.summary = message.subject
			issue.comments.append(W3DoC.Issue.Comment('Comment', message.archived_at, message))
			message.issues.append(issue)
			return issue

		@staticmethod
		def load(file):
			line = file.readline()
			if not line:
				return None
			match = re.match(r'Issue (\d+)', line)
			if not match:
				raise Exception('Issue must start with an issue #')
			issue = W3DoC.Issue()
			issue.id = int(match.group(1))
			for line in file:
				if line.startswith('---'):
					break
				(key, value) = re.split(r':\s*', line.strip(), 1)
				if key == 'Summary':
					issue.summary = value
				elif key == 'From':
					issue.from_name = value
				elif key == 'Comment' or key == 'Response':
					issue.comments.append(W3DoC.Issue.Comment(key, value))
				elif key == 'Open' or key == 'Closed':
					issue.status = key
					issue.resolution = value
					issue.comments[-1].is_before_status = True
				else:
					raise Exception('Unknown key "%s" ignored' % key)
			return issue

		def save(self, file):
			file.write(u'----\nIssue %d.\n' % self.id)
			for t in self.get_key_value_pairs_to_save():
				if not t[1]:
					file.write(unicode(t[0] + ':\n'))
				else:
					file.write(unicode((t[0] + ':').ljust(10) + t[1] + '\n'))

		def get_key_value_pairs_to_save(self):
			yield ('Summary', self.summary)
			yield ('From', self.from_name)
			is_status_yielded = False
			for comment in self.comments:
				yield (comment.type, comment.url)
				if comment.is_before_status:
					yield (self.status, self.resolution)
					is_status_yielded = True
			if not is_status_yielded and self.status:
				yield (self.status, self.resolution)

		def link_messages(self, list):
			for comment in self.comments:
				msg = list.message_by_url(comment.url)
				comment.message = msg
				if not msg:
					logger.warn('Issue %d: %s %s not loaded', self.id, comment.type, comment.url)
					continue
				msg.issues.append(self)

		def add_messages(self, list):
			last_comment = last(c for c in self.comments if c.message)
			if not last_comment:
				logger.warn('Issue %d: No messages found' % self.id)
				return
			msg = last_comment.message
			replies = self._get_messages(msg)
			for reply in sorted(replies, key=attrgetter('date')):
				logger.info('Issue %d: add comment %s' % (self.id, reply.archived_at))
				self.comments.append(W3DoC.Issue.Comment('Comment', reply.archived_at, reply))

		def _get_messages(self, m, replies =[]):
			for reply in m.replies:
				if reply.issues: # do not include messages with assigned issues in case issues were branched
					continue
				logger.info('Issue %d: add comment %s' % (self.id, reply.archived_at))
				replies.append(reply)
				self._get_messages(reply, replies)
			return replies

		class Comment(object):
			def __init__(self, type, url, message =None):
				self.type = type
				self.url = url
				self.message = message
				self.is_before_status = False

def get_redirect_url(url):
	try:
		with closing(urllib2.urlopen(url)) as response:
			redir = response.geturl()
		if redir != url:
			logger.debug('Normalized %s from %s', redir, url)
			return redir
		return None
	except urllib2.HTTPError as err:
		if err.code == 300:
			logger.debug('300 from %s', url)
			t = etree.parse(err)
			etree.tostring(t)
			hrefs = map(lambda a: a.attrib['href'], t.findall('.//li/a'))
			for href in hrefs:
				if 'www-style' in href:
					return href
			return hrefs[0]
		logger.error('HTTP %d for %s', err.code, url)
		raise
#	except:
#		err = sys.exc_info()[0]
#		logger.error('%s: %s', self.archived_at, err)

def decode_header_to_unicode(text):
	decoded = email.header.decode_header(text)
	return u''.join([unicode(text, charset or 'ascii') for text, charset in decoded])

re_angle_bracket = re.compile(r'<([^>]*)>')

def strip_angle_brackets(text):
	match = re_angle_bracket.search(text)
	if match:
		return match.group(1)
	return text

def first(iterable, default=None):
	if iterable:
		for item in iterable:
			return item
	return default

def last(iterable, default=None):
	if iterable:
		for item in iterable:
			default = item
	return default

def main():
	reload(sys).setdefaultencoding('utf-8')
	parser = argparse.ArgumentParser()
	parser.add_argument('--doc')
	parser.add_argument('--exclude', '-x', action='append')
	parser.add_argument('--filter', '-f', action='append')
	parser.add_argument('--mbox', '-m', default='~/Downloads')
	parser.add_argument('--redir', '-r', default='redir.json')
	parser.add_argument('--since', '-s')
	parser.add_argument('--user', '-u',)
	parser.add_argument('--verbose', '-v', action='append_const', const=1)
#	parser.add_argument('url', nargs='*')
	args = parser.parse_args()
	if args.verbose:
		logger.setLevel(level=logging.INFO if len(args.verbose) == 1 else logging.DEBUG)
	args.mbox = os.path.expanduser(args.mbox)
	list = W3lists()
	if args.user:
		r = list.month_index_range_to_download(args.mbox, args.since)
		if r:
			(user, password) = args.user.split(':')
			list.download_mbox(args.mbox, r, user, password)
		else:
			info.Warning('Downloading mbox skipped: you need to have at least one mbox file to download')
	if args.redir:
		args.redir = os.path.expanduser(os.path.join(args.mbox, args.redir))
		if os.path.exists(args.redir):
			list.load_urls(args.redir)
	if args.filter:
		f = re.compile('|'.join(args.filter), re.I)
		if args.exclude:
			x = re.compile('|'.join(args.exclude), re.I)
			filter = lambda m: f.search(m.subject) and not x.search(m.subject)
		else:
			filter = lambda m: f.search(m.subject)
	else:
		filter = None
	if args.since:
		list.load_mbox_since(args.mbox, args.since, filter)
	else:
		list.load_mbox(args.mbox, filter)
	list.link_replies()
	if args.redir:
		list.save_urls(args.redir)
	if args.doc:
		doc = W3DoC()
		doc.load(args.doc)
		doc.link_messages(list)
		doc.add_issues(list)
		doc.add_messages(list)
		doc.save(args.doc)
	else:
		list.dump_reply_tree(sys.stdout)
	logger.info("Completed")

main()
