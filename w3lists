#!/usr/bin/env python
# Create thread trees by following in-reply-to links from lists.w3.org, such as
# http://lists.w3.org/Archives/Public/www-style/2014Apr/
# https://lists.w3.org/Archives/Public/${list}/mboxes/${year}-${month}.mbx
# https://lists.w3.org/Archives/Public/www-style/mboxes/2014-05.mbx
import argparse
import contextlib
import datetime
import email
import glob
import io
import json
import logging
import mailbox
from operator import attrgetter
import os
import re
import rfc822
import urllib2
import sys
from xml.sax.saxutils import escape
#from xml.etree import ElementTree
from lxml import etree
logger = logging.getLogger('w3lists')
logging.basicConfig()

class w3lists(object):
	def __init__(self):
		self.message_by_id = {}
		self.normalized_url = {}

	def load_mbox(self, path, filter =None):
		path = os.path.expanduser(path)
		if os.path.isdir(path):
			for file in glob.glob(os.path.join(path, '*.mbx')):
				self.load_mbox(file, filter)
			return
		logger.info('Loading %s', path)
		mbox = mailbox.mbox(path)
		for mbmsg in mbox:
			m = w3lists.message(mbmsg)
			if filter and not filter(m):
				continue
			m.archived_at = self.normalize_url(m.archived_at)
			self.message_by_id[m.id] = m

	def load_urls(self, path):
		logger.info('Loading URLs from %s', path)
		with open(path, 'r') as f:
			self.normalized_url = json.load(f)

	def save_urls(self, path):
		logger.info('Saving URLs to %s', path)
		with open(path, 'w') as f:
			json.dump(self.normalized_url, f, indent=0)

	def normalize_url(self, url):
		normalized = self.normalized_url.get(url)
		if normalized:
			return normalized
		try:
			redir = get_redirect_url(url)
			if not redir:
				return url
		except urllib2.HTTPError as err:
			if err.code == 404:
				return url
			raise
		self.normalized_url[url] = redir
		return redir

	def link_replies(self):
		logger.info("Linking replies...")
		for m in self.message_by_id.itervalues():
			m.link_in_reply_to(self.message_by_id)
		self.link_replies_by_subject()

	def link_replies_by_subject(self):
		logger.info("Linking replies using subjects...")
		message_by_subject = {}
		re_subject_prefix = re.compile('^[a-zA-Z]{2,3}:\s*')
		def normalize_subject(subject):
			subject = re_subject_prefix.sub('', subject)
			subject = subject.replace(' ', '')
			return subject
		for m in self.message_by_id.itervalues():
			subject = normalize_subject(m.subject)
			thread = message_by_subject.get(subject)
			if thread:
				thread.append(m)
			else:
				message_by_subject[subject] = [m]
		for m in filter(lambda m: not m.in_reply_to, self.message_by_id.itervalues()):
			subject = m.subject
			if not re_subject_prefix.match(subject):
				continue
			subject = normalize_subject(subject)
			thread = message_by_subject.get(subject)
			assert thread
			earliers = filter(lambda mm: mm.date < m.date, thread)
			if not earliers:
				logger.info('Link maybe-by-subject candidates not found: %s from %s on %s', m.subject, m.from_display, m.date)
				continue
			in_reply_to = first(reversed(sorted(earliers, key=attrgetter('date'))))
			assert normalize_subject(m.subject) == normalize_subject(in_reply_to.subject)
			logger.info('Linking maybe-by-subject: %s from %s on %s', m.subject, m.from_display, m.date)
			in_reply_to.add_reply(m)
			assert m.in_reply_to

	def dump_reply_tree(self, output):
		output.write('''<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
</head>
<body>
<ol>''')
		for m in sorted(filter(lambda m: m.in_reply_to is None, self.message_by_id.itervalues()), key=attrgetter('date')):
			m.dump_reply_tree(output)
		output.write('</ol></body></html>\n')

	class message(object):
		def __init__(self, mbmsg):
			self.archived_at = strip_angle_brackets(mbmsg['archived-at'])
			self.date = datetime.datetime.fromtimestamp(rfc822.mktime_tz(rfc822.parsedate_tz(mbmsg['date'])))
			(self.from_display, self.from_address) = rfc822.parseaddr(decode_header_to_unicode(mbmsg['from']))
			self.id = email.Utils.parseaddr(mbmsg['message-id'])[1]
			self.in_reply_to_id = mbmsg['in-reply-to']
			if self.in_reply_to_id:
				self.in_reply_to_id = email.Utils.parseaddr(self.in_reply_to_id)[1]
			self.subject = mbmsg['subject']
			self.in_reply_to = None
			self.replies = []

		def link_in_reply_to(self, message_by_id):
			assert not self.in_reply_to
			in_reply_to_id = self.in_reply_to_id
			if not in_reply_to_id:
				return
			in_reply_to = message_by_id.get(in_reply_to_id)
			if not in_reply_to:
				logger.warn('Message id %s not loaded', in_reply_to_id)
				return
			in_reply_to.add_reply(self)

		def add_reply(self, reply):
			assert not reply.in_reply_to
			self.replies.append(reply)
			reply.in_reply_to = self

		def dump_reply_tree(self, output):
			output.write('\n<li><a href="{href}">{subject}</a> {from_display} {date}'.format(
				date=self.date,
				from_display=escape(self.from_display),
				href=self.archived_at,
				subject=escape(self.subject)))
			if self.replies:
#				output.write(' %d replies' % len(self.replies))
				output.write('<ol>')
				for r in sorted(self.replies, key=attrgetter('date')):
					r.dump_reply_tree(output)
				output.write('</ol>')
			output.write('</li>')

def get_redirect_url(url):
	try:
		with contextlib.closing(urllib2.urlopen(url)) as response:
			redir = response.geturl()
		if redir != url:
			logger.debug('Normalized %s from %s', redir, url)
			return redir
		return None
	except urllib2.HTTPError as err:
		if err.code == 300:
			logger.debug('300 from %s', url)
			t = etree.parse(err)
			etree.tostring(t)
			hrefs = map(lambda a: a.attrib['href'], t.findall('.//li/a'))
			for href in hrefs:
				if 'www-style' in href:
					return href
			return hrefs[0]
		logger.error('HTTP %d for %s', err.code, url)
		raise
#	except:
#		err = sys.exc_info()[0]
#		logger.error('%s: %s', self.archived_at, err)

def decode_header_to_unicode(text):
	decoded = email.header.decode_header(text)
	return u''.join([unicode(text, charset or 'ascii') for text, charset in decoded])

re_angle_bracket = re.compile(r'<([^>]*)>')

def strip_angle_brackets(text):
	match = re_angle_bracket.search(text)
	if match:
		return match.group(1)
	return text

def first(iterable, default=None):
	if iterable:
		for item in iterable:
			return item
	return default

def main():
	reload(sys).setdefaultencoding('utf-8')
	parser = argparse.ArgumentParser(description='lists.w3.org scraper')
	parser.add_argument('--filter', '-f')
	parser.add_argument('--mbox', '-m', default='~/Downloads')
	parser.add_argument('--redir', '-r', default='redir.json')
	parser.add_argument('--verbose', '-v', action='append_const', const=1)
#	parser.add_argument('url', nargs='*')
	args = parser.parse_args()
	if args.verbose:
		logger.setLevel(level=logging.INFO if len(args.verbose) == 1 else logging.DEBUG)
	l = w3lists()
	if args.redir:
		args.redir = os.path.expanduser(os.path.join(args.mbox, args.redir))
		if os.path.exists(args.redir):
			l.load_urls(args.redir)
	l.load_mbox(args.mbox, (lambda m: re.search(args.filter, m.subject, re.I)) if args.filter else None)
	l.link_replies()
	if args.redir:
		l.save_urls(args.redir)
	l.dump_reply_tree(sys.stdout)
	logger.info("Completed")

main()
